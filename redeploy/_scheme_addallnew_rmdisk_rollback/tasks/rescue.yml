---

- debug: msg="Rescuing"

- name: rescue | re-acquire cluster_hosts_target and cluster_hosts_state
  import_role:
    name: clusterverse/cluster_hosts

- name: rescue | Reset lifecycle_state labels
  block:
    - name: rescue | Change lifecycle_state label from 'current' to 'redeployfail'
      include_role:
        name: clusterverse/redeploy/__common
        tasks_from: "set_lifecycle_state_label_{{cluster_vars.type}}.yml"
      vars:
        hosts_to_relabel: "{{ cluster_hosts_state | json_query(\"[?tagslabels.lifecycle_state=='current']\") }}"
        new_state: "redeployfail"
      when: "'current' in (cluster_hosts_state | map(attribute='tagslabels.lifecycle_state'))  and  'retiring' in (cluster_hosts_state | map(attribute='tagslabels.lifecycle_state'))"

    - name: rescue | Change lifecycle_state label from 'retiring' to 'current' state
      include_role:
        name: clusterverse/redeploy/__common
        tasks_from: "set_lifecycle_state_label_{{cluster_vars.type}}.yml"
      vars:
        hosts_to_relabel: "{{ cluster_hosts_state | json_query(\"[?tagslabels.lifecycle_state=='retiring']\") }}"
        new_state: "current"
      when: "'retiring' in (cluster_hosts_state | map(attribute='tagslabels.lifecycle_state'))"


- name: rescue | re-acquire cluster_hosts_target and cluster_hosts_state
  import_role:
    name: clusterverse/cluster_hosts

- name: "rescue | Run {{mainclusteryml}} to perform readiness steps on old cluster (maintenance_mode, CNAME).  Send cluster_hosts_target that maps to cluster_hosts_state, because the topology might have changed, and should only set CNAMEs back for original hosts, not those in cluster_vars."
  shell: "{{ (argv | join(' ')) | regex_replace('redeploy.yml', mainclusteryml) }} -e '{'cluster_hosts_target': {{_cluster_hosts_target_prev | to_json}}}' --tags=clusterverse_dynamic_inventory,clusterverse_readiness"
  vars:
    _cluster_hosts_state_current: "{{ cluster_hosts_state | json_query(\"[?tagslabels.lifecycle_state=='current'].name\") }}"
    _cluster_hosts_target_prev: "{{ cluster_hosts_target | json_query(\"[?contains(`\" + _cluster_hosts_state_current | join(',') + \"`, hostname)]\") }}"
  register: r__mainclusteryml
  no_log: True
  ignore_errors: yes
- debug: msg="{{[r__mainclusteryml.stdout_lines] + [r__mainclusteryml.stderr_lines]}}"
  failed_when: r__mainclusteryml is failed
  when: r__mainclusteryml is failed  or  (debug_nested_log_output is defined and debug_nested_log_output|bool)

- name: rescue | re-acquire cluster_hosts_target and cluster_hosts_state
  import_role:
    name: clusterverse/cluster_hosts

- name: rescue | run predeleterole role
  include_role:
    name: "{{predeleterole}}"
  vars:
    hosts_to_remove: "{{ cluster_hosts_state | json_query(\"[?tagslabels.lifecycle_state=='redeployfail']\") }}"
  when: predeleterole is defined and predeleterole != ""

- name: rescue | poweroff the failed VMs
  include_role:
    name: clusterverse/redeploy/__common
    tasks_from: "powerchange_vms_{{cluster_vars.type}}.yml"
  when: hosts_to_powerchange | length
  vars:
    hosts_to_powerchange: "{{ cluster_hosts_state | json_query(\"[?tagslabels.lifecycle_state=='redeployfail']\") }}"
    powerchange_new_state: "stop"
