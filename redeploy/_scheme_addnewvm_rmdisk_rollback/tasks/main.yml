---

- name: Preflight check
  block:
    - assert: { that: "non_current_hosts | length == 0", msg: "ERROR - All VMs must be in the 'current' lifecycle_state.  Those not [{{non_current_hosts | join(',')}}]"  }
      vars:
        non_current_hosts: "{{ cluster_hosts_state | json_query(\"[?tagslabels.lifecycle_state!='current'].name\") }}"
        # TODO: remove myhosttypes not defined and replace json_query    "{{ cluster_hosts_state | json_query(\"[?tagslabels.lifecycle_state!='current' && ('\"+ myhosttypes|default('') + \"' == ''  ||  contains(['\"+ myhosttypes|default('') + \"'], tagslabels.hosttype))].name\") }}"
      when: (canary=="start" or canary=="none") and (myhosttypes is not defined or myhosttypes=='')


- name: Redeploy by hosttype; rollback on fail
  block:
    - name: Change lifecycle_state label from 'current' to 'retiring'
      include_role:
        name: clusterverse/redeploy/__common
        tasks_from: "set_lifecycle_state_label_{{cluster_vars.type}}.yml"
      vars:
        hosts_to_relabel: "{{ cluster_hosts_state | json_query(\"[?tagslabels.lifecycle_state=='current']\") }}"
        new_state: "retiring"
      when: (canary=="start" or canary=="none") and ('retiring' not in (cluster_hosts_state | map(attribute='tagslabels.lifecycle_state')))
      #TODO: Can probably remove the ' and ('retiring' not in (cluster_hosts_state | map(attribute='tagslabels.lifecycle_state')))'

    - name: re-acquire cluster_hosts_target and cluster_hosts_state
      import_role:
        name: clusterverse/cluster_hosts

    - name: Run redeploy per hosttype.  Create one at a time, then stop previous.
      include_tasks: redeploy_by_hosttype.yml
      with_items: "{{ myhosttypes_array | default([]) }}"
      loop_control:
        loop_var: hosttype
      vars:
        cluster_hosts_target_by_hosttype: "{{cluster_hosts_target | default({}) | dict_agg('hosttype') }}"
        myhosttypes_array: "{%- if myhosttypes is defined -%}{{ myhosttypes.split(',') }}{%- elif cluster_hosts_target_by_hosttype is defined and cluster_hosts_target_by_hosttype.keys() | length -%}{{ cluster_hosts_target_by_hosttype.keys() | list }}{%- else -%}[]{%- endif -%}"

    - name: Remove any other retiring VM(s) that might exist if we're redeploying to a smaller topology.
      block:
        - name: run predeleterole role on any other retiring VM(s) that might exist if we're redeploying to a smaller topology.
          include_role:
            name: "{{predeleterole}}"
          when: predeleterole is defined and predeleterole != ""
          vars:
            hosts_to_remove: "{{ hosts_to_change | json_query(\"[?contains('RUNNING,running,poweredOn', instance_state)]\") }}"

        - name: Power off any other retiring VM(s) that might exist if we're redeploying to a smaller topology.
          include_role:
            name: clusterverse/redeploy/__common
            tasks_from: "powerchange_vms_{{cluster_vars.type}}.yml"
          vars:
            hosts_to_powerchange: "{{ hosts_to_change }}"
            powerchange_new_state: "stop"
      vars:
        hosts_to_change: "{{ cluster_hosts_state | json_query(\"[?tagslabels.lifecycle_state=='retiring' && ('\"+ myhosttypes|default('') + \"' == ''  ||  contains(['\"+ myhosttypes|default('') + \"'], tagslabels.hosttype))]\") }}"
      when: (canary=="finish" or canary=="none")

    - name: re-acquire cluster_hosts_target and cluster_hosts_state (for tidy - can't be in the tidy block because the block depends on this info being correct)
      include_role:
        name: clusterverse/cluster_hosts
      when: (canary_tidy_on_success is defined and canary_tidy_on_success|bool)

  rescue:
    - debug: msg="Rescuing"

    - name: rescue
      include_tasks: rescue.yml

    - name: rescue | force fail from block so Jenkins gets error code.  (If we prefer not to fail, we should otherwise 'meta end_play' to prevent tidying of pre-rescued VMs)
      fail: { msg: ["Task '{{ansible_failed_task.name}}' failed in block (but rescued)", "{{ansible_failed_result.msg}}"] }
  when: canary!="tidy"


- name: "Tidy up powered-down, non-current instances.  NOTE: Must do clean_dns first, because both clean_dns and clean_vms have the cluster_hosts role as a dependency, which when run after clean_vms, will be empty."
  block:
    - assert: { that: "'current' in (cluster_hosts_state | map(attribute='tagslabels.lifecycle_state'))", msg: "ERROR - Cannot tidy when there are no machines in the 'current' lifecycle_state.  Please use '-e clean=_all_'." }

    - include_role:
        name: clusterverse/clean
        tasks_from: dns.yml
      when: (hosts_to_clean | length)  and  (cluster_vars.dns_server is defined and cluster_vars.dns_server != "") and (cluster_vars.dns_user_domain is defined and cluster_vars.dns_user_domain != "")

    - include_role:
        name: clusterverse/clean
        tasks_from: "{{cluster_vars.type}}.yml"
      when: (hosts_to_clean | length)

    - debug:
        msg: "tidy | No hosts to tidy.  Only powered-down, non-current machines with be tidied; to clean other machines, please use the '-e clean=<state>' extra variable."
      when: hosts_to_clean | length == 0
  vars:
    hosts_to_clean: "{{ cluster_hosts_state | json_query(\"[?tagslabels.lifecycle_state!='current' && !(contains('RUNNING,running,poweredOn', instance_state)) && ('\"+ canary + \"' == 'tidy'  ||  '\"+ myhosttypes|default('') + \"' == ''  ||  contains(['\"+ myhosttypes|default('') + \"'], tagslabels.hosttype)) ]\") }}"
  when: canary=="tidy" or  ((canary=="none" or canary=="finish") and canary_tidy_on_success is defined and canary_tidy_on_success|bool)
