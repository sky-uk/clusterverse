---

- name: Include preflight checks/ assertions.
  include_tasks: preflight.yml

- name: Redeploy by hosttype; rollback on fail
  block:
    - name: Redeploy setup
      block:
        - name: Change lifecycle_state label from 'current' to 'retiring'
          include_role:
            name: clusterverse/redeploy/__common
            tasks_from: set_lifecycle_state_label.yml
          vars:
            hosts_to_relabel: "{{ cluster_hosts_state | json_query(\"[?tagslabels.lifecycle_state=='current']\") }}"
            new_state: "retiring"
          when: ('retiring' not in (cluster_hosts_state | map(attribute='tagslabels.lifecycle_state')))

        - name: re-acquire cluster_hosts_target and cluster_hosts_state
          include_role:
            name: clusterverse/cluster_hosts
            public: yes

        - assert: { that: "cluster_hosts_state | json_query(\"[?tagslabels.cluster_suffix == '\"+ cluster_suffix +\"' && ('\"+ myhosttypes|default('') + \"' == ''  ||  contains(['\"+ myhosttypes|default('') + \"'], tagslabels.hosttype))]\") | length == 0", msg: "Please ensure cluster_suffix ({{cluster_suffix}}) is not already set on the cluster" }
          when: cluster_suffix is defined
      when: (canary=="start" or canary=="none")

    - name: Add the disk info from previous instances to cluster_hosts_target
      include_tasks: "_add_src_diskinfo_to_cluster_hosts_target__{{cluster_vars.type}}.yml"

    - name: Run redeploy per hosttype.  Create one at a time, then stop previous.
      include_tasks: by_hosttype.yml
      with_items: "{{ myhosttypes_array }}"
      loop_control:
        loop_var: hosttype
      vars:
        cluster_hosts_target_by_hosttype: "{{cluster_hosts_target | dict_agg('hosttype')}}"
        myhosttypes_array: "{%- if myhosttypes is defined -%} {{ myhosttypes.split(',') }} {%- else -%} {{ cluster_hosts_target_by_hosttype.keys() | list }} {%- endif -%}"

    - fail:
      when: testfail is defined and testfail == "fail_1"

    - name: re-acquire cluster_hosts_target and cluster_hosts_state (For the '-e canary=tidy' option. This can't be run in the tidy block below because that block depends on this info being correct)
      import_role:
        name: clusterverse/cluster_hosts
      when: (canary_tidy_on_success is defined and canary_tidy_on_success|bool)

  rescue:
    - debug: msg="Rescuing"

    - name: rescue | re-acquire cluster_hosts_target and cluster_hosts_state
      import_role:
        name: clusterverse/cluster_hosts

    - name: rescue | Change lifecycle_state label from 'current' to 'redeployfail'
      include_role:
        name: clusterverse/redeploy/__common
        tasks_from: set_lifecycle_state_label.yml
      vars:
        hosts_to_relabel: "{{ cluster_hosts_state | json_query(\"[?tagslabels.lifecycle_state=='current']\") }}"
        new_state: "redeployfail"

    - name: rescue | Change lifecycle_state label from 'retiring' to 'current'
      include_role:
        name: clusterverse/redeploy/__common
        tasks_from: set_lifecycle_state_label.yml
      vars:
        hosts_to_relabel: "{{ cluster_hosts_state | json_query(\"[?tagslabels.lifecycle_state=='retiring']\") }}"
        new_state: "current"

    - name: rescue | re-acquire cluster_hosts_target and cluster_hosts_state
      import_role:
        name: clusterverse/cluster_hosts

    - name: rescue | Add the disk info from previous instances to cluster_hosts_target
      include_tasks: "_add_src_diskinfo_to_cluster_hosts_target__{{cluster_vars.type}}.yml"

    - name: rescue | explicitly specify only the relevant cluster.yml roles to run for rescuing
      set_fact:
        argv: "{{argv + ['--tags'] + ['clusterverse_create,clusterverse_dynamic_inventory,clusterverse_readiness'] }}"

    - name: rescue | Run redeploy per hosttype.  Create one at a time, then stop previous.
      include_tasks: by_hosttype.yml
      with_items: "{{ myhosttypes_array }}"
      loop_control:
        loop_var: hosttype
      vars:
        cluster_hosts_target_by_hosttype: "{{cluster_hosts_target | dict_agg('hosttype')}}"
        myhosttypes_array: "{%- if myhosttypes is defined -%} {{ myhosttypes.split(',') }} {%- else -%} {{ cluster_hosts_target_by_hosttype.keys() | list }} {%- endif -%}"

    - name: rescue | force fail from block so Jenkins gets error code.  (If we prefer not to fail, we should otherwise 'meta end_play' to prevent tidying of pre-rescued VMs)
      fail: { msg: "Task '{{ansible_failed_task.name}}' failed in block.    Error message was: '{{ansible_failed_result.msg}}'" }
  when: canary!="tidy"


- name: "Tidy up powered-down, non-current instances.  NOTE: Must do clean_dns first, because both clean_dns and clean_vms have the cluster_hosts role as a dependency, which when run after clean_vms, will be empty."
  block:
    - assert: { that: "'current' in (cluster_hosts_state | map(attribute='tagslabels.lifecycle_state'))", msg: "ERROR - Cannot tidy when there are no machines in the 'current' lifecycle_state.  Please use '-e clean=_all_'." }

    - include_role:
        name: clusterverse/clean
        tasks_from: clean_dns.yml
      when: (hosts_to_clean | length)  and  (cluster_vars.dns_server is defined and cluster_vars.dns_server != "") and (cluster_vars.dns_user_domain is defined and cluster_vars.dns_user_domain != "")

    - include_role:
        name: clusterverse/clean
        tasks_from: clean_vms.yml
      when: (hosts_to_clean | length)

    - debug:
        msg: "tidy | No hosts to tidy.  Only powered-down, non-current machines with be tidied; to clean other machines, please use the '-e clean=<state>' extra variable."
      when: hosts_to_clean | length == 0
  vars:
    hosts_to_clean: "{{ cluster_hosts_state | json_query(\"[?tagslabels.lifecycle_state!='current' && !(contains('RUNNING,running,poweredOn', instance_state)) && ('\"+ canary + \"' == 'tidy'  ||  '\"+ myhosttypes|default('') + \"' == ''  ||  contains(['\"+ myhosttypes|default('') + \"'], tagslabels.hosttype)) ]\") }}"
  when: canary=="tidy" or  ((canary=="none" or canary=="finish") and canary_tidy_on_success is defined and canary_tidy_on_success|bool)
