---

# GCP credentials
gcp_credentials_file: "{{ lookup('env','GCP_CREDENTIALS') | default('/dev/null', true) }}"
gcp_credentials_json: "{{ lookup('file', gcp_credentials_file) | default({'project_id': 'GCP_CREDENTIALS__NOT_SET','client_email': 'GCP_CREDENTIALS__NOT_SET'}, true) }}"

#redeploy_scheme: _scheme_addallnew_rmdisk_rollback
#redeploy_scheme: _scheme_addnewvm_rmdisk_rollback
#redeploy_scheme: _scheme_rmvm_rmdisk_only

app_name: "test"                  # The name of the application cluster (e.g. 'couchbase', 'nginx'); becomes part of cluster_name.
app_class: "test"                 # The class of application (e.g. 'database', 'webserver'); becomes part of the fqdn

beats_config:
  filebeat:
#    output_logstash_hosts: ["localhost:5044"]   # The destination hosts for filebeat-gathered logs
#    extra_logs_paths: # The array is optional, if you need to add more paths or files to scrape for logs
#      - /var/log/myapp/*.log 
  metricbeat:
#    output_logstash_hosts: ["localhost:5044"]   # The destination hosts for metricbeat-gathered metrics

## Vulnerability scanners - Tenable and/ or Qualys cloud agents:
cloud_agent:
#  tenable:
#    service: "nessusagent"
#    debpackage: ""
#    bin_path: "/opt/nessus_agent/sbin"
#    nessus_key_id: ""
#    nessus_group_id: ""
#    proxy: {host: "", port: ""}
#  qualys:
#    service: "qualys-cloud-agent"
#    debpackage: ""
#    bin_path: "/usr/local/qualys/cloud-agent/bin"
#    config_path: "/etc/default/qualys-cloud-agent"
#    activation_id: ""
#    customer_id: ""
#    proxy: {host: "", port: ""}

## Bind configuration and credentials, per environment
bind9:
  sandbox: {server: "", key_name: "", key_secret: ""}

cluster_name: "{{app_name}}-{{buildenv}}"       # Identifies the cluster within the cloud environment

cluster_vars:
  type: &cloud_type "gcp"
  image: "projects/ubuntu-os-cloud/global/images/ubuntu-1804-bionic-v20200430"  # Ubuntu images can be located at https://cloud-images.ubuntu.com/locator/
  region: &region "europe-west1"
  dns_cloud_internal_domain: "c.{{gcp_credentials_json.project_id}}.internal"   # The cloud-internal zone as defined by the cloud provider (e.g. GCP, AWS)
  dns_nameserver_zone: &dns_nameserver_zone ""                                  # The zone that dns_server will operate on.  gcloud dns needs a trailing '.'.  Leave blank if no external DNS (use IPs only)
  dns_user_domain: "{%- if _dns_nameserver_zone -%}{{_cloud_type}}-{{_region}}.{{app_class}}.{{buildenv}}.{{_dns_nameserver_zone}}{%- endif -%}"         # A user-defined _domain_ part of the FDQN, (if more prefixes are required before the dns_nameserver_zone)
  dns_server: ""                            # Specify DNS server. nsupdate, route53 or clouddns.  If empty string is specified, no DNS will be added.
  assign_public_ip: "yes"
  inventory_ip: "public"                    # 'public' or 'private', (private in case we're operating in a private LAN).  If public, 'assign_public_ip' must be 'yes'
  ip_forward: "false"
  ssh_guard_whitelist: &ssh_guard_whitelist ['10.0.0.0/8']    # Put your public-facing IPs into this (if you're going to access it via public IP), to avoid rate-limiting.
  custom_tagslabels:
    inv_resident_id: "myresident"
    inv_proposition_id: "myproposition"
    inv_environment_id: "{{buildenv}}"
    inv_service_id: "{{app_class}}"
    inv_cluster_id: "{{cluster_name}}"
    inv_cluster_type: "{{app_name}}"
    inv_cost_centre: "1234"
  network_fw_tags: ["{{cluster_name}}-nwtag"]
  firewall_rules:
    - name: "{{cluster_name}}-extssh"
      allowed: [{ip_protocol: "tcp", ports: ["22"]}]
      source_ranges: "{{_ssh_guard_whitelist}}"
      description: "SSH Access"
    - name: "{{cluster_name}}-prometheus-node-exporter"
      allowed: [{ip_protocol: "tcp", ports: ["{{ prometheus_node_exporter_port | default(9100) }}"]}]
      source_tags: ["{{cluster_name}}-nwtag"]
      description: "Prometheus instances attached to {{cluster_name}}-nwtag can access the exporter port(s)."
    - name: "{{cluster_name}}-nwtag"
      allowed: [{ip_protocol: "all"}]
      source_tags: ["{{cluster_name}}-nwtag"]
      description: "Access from all VMs attached to the {{cluster_name}}-nwtag group"
  sandbox:
    hosttype_vars:
      sys: {vms_by_az: {b: 1, c: 1, d: 1}, flavor: f1-micro, rootvol_size: "10", version: "{{sys_version | default('')}}", auto_volumes: []}
      #sysdisks: {vms_by_az: {b: 1, c: 1, d: 1}, flavor: f1-micro, rootvol_size: "10", version: "{{sysdisks_version | default('')}}", auto_volumes: [{auto_delete: true, interface: "SCSI", volume_size: 2, mountpoint: "/var/log/mysvc", fstype: "ext4", perms: {owner: "root", group: "sudo", mode: "775"}}, {auto_delete: true, interface: "SCSI", volume_size: 2,  mountpoint: "/var/log/mysvc2", fstype: "ext4"}, {auto_delete: true, interface: "SCSI", volume_size: 3,  mountpoint: "/var/log/mysvc3", fstype: "ext4"}]}
    vpc_project_id: "{{gcp_credentials_json.project_id}}"             # AKA the 'service project' if Shared VPC (https://cloud.google.com/vpc/docs/shared-vpc) is in use.
    vpc_host_project_id: "{{gcp_credentials_json.project_id}}"        # Would differ from vpc_project_id if Shared VPC is in use, (the networking is in a separate project)
    vpc_network_name: "test-{{buildenv}}"
    vpc_subnet_name: ""
    preemptible: "no"
    deletion_protection: "no"
_cloud_type: *cloud_type
_region: *region
_ssh_guard_whitelist: *ssh_guard_whitelist
_dns_nameserver_zone: *dns_nameserver_zone
