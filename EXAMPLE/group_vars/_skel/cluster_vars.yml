---

# GCP credentials
gcp_credentials_file: "{{ lookup('env','GCP_CREDENTIALS') | default('/dev/null', true) }}"
gcp_credentials_json: "{{ lookup('file', gcp_credentials_file) | default({'project_id': 'GCP_CREDENTIALS__NOT_SET','client_email': 'GCP_CREDENTIALS__NOT_SET'}, true) }}"

app_name: "test"                  # The name of the application cluster (e.g. 'couchbase', 'nginx'); becomes part of cluster_name.
app_class: "test"                 # The class of application (e.g. 'database', 'webserver'); becomes part of the fqdn

beats_config:
  filebeat:
#    output_logstash_hosts: ["localhost:5044"]   # The destination hosts for filebeat-gathered logs
#    extra_logs_paths: # The array is optional, if you need to add more paths or files to scrape for logs
#      - /var/log/myapp/*.log
  metricbeat:
#    output_logstash_hosts: ["localhost:5044"]   # The destination hosts for metricbeat-gathered metrics

## Vulnerability scanners - Tenable and/ or Qualys cloud agents:
cloud_agent:
#  tenable:
#    service: "nessusagent"
#    debpackage: ""
#    bin_path: "/opt/nessus_agent/sbin"
#    nessus_key_id: ""
#    nessus_group_id: ""
#    proxy: {host: "", port: ""}
#  qualys:
#    service: "qualys-cloud-agent"
#    debpackage: ""
#    bin_path: "/usr/local/qualys/cloud-agent/bin"
#    config_path: "/etc/default/qualys-cloud-agent"
#    activation_id: ""
#    customer_id: ""
#    proxy: {host: "", port: ""}

## Bind configuration and credentials, per environment
bind9:
  sandbox: {server: "", key_name: "", key_secret: ""}

cluster_name: "{{app_name}}-{{buildenv}}"       # Identifies the cluster within the cloud environment

### AWS example
#cluster_vars:
#  type: &cloud_type "aws"
#  image: "ami-0c4c42893066a139e"            # eu-west-1, 20.04, amd64, hvm-ssd, 20200924.  Ubuntu images can be located at https://cloud-images.ubuntu.com/locator/
#  image: "ami-06868ad5a3642e4d7"            # eu-west-1, 18.04, amd64, hvm-ssd, 20200923.  Ubuntu images can be located at https://cloud-images.ubuntu.com/locator/
#  region: &region "eu-west-1"       # eu-west-1, us-west-2
#  dns_cloud_internal_domain: "{{_region}}.compute.internal"                       # The cloud-internal zone as defined by the cloud provider (e.g. GCP, AWS)
#  dns_nameserver_zone: &dns_nameserver_zone ""                                    # The zone that dns_server will operate on.  gcloud dns needs a trailing '.'.  Leave blank if no external DNS (use IPs only)
#  dns_user_domain: "{%- if _dns_nameserver_zone -%}MY.OTHER.PREFIXES.{{_dns_nameserver_zone}}{%- endif -%}"         # A user-defined _domain_ part of the FDQN, (if more prefixes are required before the dns_nameserver_zone)
#  dns_server: ""                    # Specify DNS server. nsupdate, route53 or clouddns.  If empty string is specified, no DNS will be added.
#  route53_private_zone: true        # Only used when cluster_vars.type == 'aws'. Defaults to true if not set.
#  assign_public_ip: "yes"
#  inventory_ip: "public"            # 'public' or 'private', (private in case we're operating in a private LAN).  If public, 'assign_public_ip' must be 'yes'
#  instance_profile_name: ""
#  custom_tagslabels: {inv_resident_id: "abc", inv_proposition_id: "def"}
#  secgroups_existing: []
#  secgroup_new:
#    - proto: "tcp"
#      ports: ["22"]
#      cidr_ip: "0.0.0.0/0"
#      rule_desc: "SSH Access"
#    - proto: "tcp"
#      ports: ["{{ prometheus_node_exporter_port | default(9100) }}"]
#      group_name: ["{{buildenv}}-private-sg"]
#      rule_desc: "Prometheus instances attached to {{buildenv}}-private-sg can access the exporter port(s)."
#    - proto: all
#      group_name: ["{{cluster_name}}-sg"]
#      rule_desc: "Access from all VMs attached to the {{ cluster_name }}-sg group"
#  sandbox:
#    hosttype_vars:
#      sys: {vms_by_az: {a: 1, b: 1, c: 1}, flavor: t3a.nano, version: "{{sys_version | default('')}}", auto_volumes: []}
#      # sysnobeats: {vms_by_az: {a: 1, b: 0, c: 0}, skip_beat_install:true, flavor: t3a.nano, version: "{{sysnobeats_version | default('')}}", auto_volumes: []
#      # sysdisks: {vms_by_az: {a: 1, b: 0, c: 0}, flavor: t3a.nano, version: "{{sysdisks_version | default('')}}", auto_volumes: [{"device_name": "/dev/sdb", mountpoint: "/var/log/mysvc", fstype: "ext4", "volume_type": "gp2", "volume_size": 2, ephemeral: False, encrypted: True, "delete_on_termination": true, perms: {owner: "root", group: "sudo", mode: "775"} }, {"device_name": "/dev/sdc", mountpoint: "/var/log/mysvc2", fstype: "ext4", "volume_type": "gp2", "volume_size": 3, ephemeral: False, encrypted: True, "delete_on_termination": true}, {"device_name": "/dev/sdd", mountpoint: "/var/log/mysvc3", fstype: "ext4", "volume_type": "gp2", "volume_size": 2, ephemeral: False, encrypted: True, "delete_on_termination": true}]}
#      # sysdisks_snapshot: {vms_by_az: {a: 1, b: 1, c: 0}, flavor: t3a.nano, auto_volumes: [{"snapshot_tags": {"tag:backup_id": "57180566894481854905"}, "device_name": "/dev/sdb", mountpoint: "/data", fstype: "ext4", "volume_type": "gp2", "volume_size": 2, ephemeral: False, encrypted: True, "delete_on_termination": true }]}
#      # hostnvme_multi: {vms_by_az: {a: 1, b: 0, c: 0}, flavor: i3en.2xlarge, auto_volumes: [], nvme: {volumes: [{mountpoint: "/var/log/mysvc", fstype: ext4, volume_size: 2500}, {mountpoint: "/var/log/mysvc2", fstype: ext4, volume_size: 2500}]} }
#      # hostnvme_lvm: {vms_by_az: {a: 1, b: 0, c: 0}, flavor: i3en.2xlarge, auto_volumes: [], nvme: {volumes: [{mountpoint: "/var/log/mysvc", fstype: ext4, volume_size: 2500}, {mountpoint: "/var/log/mysvc", fstype: ext4, volume_size: 2500}], lvmparams: {vg_name: "vg0", lv_name: "lv0", lv_size: "+100%FREE"} } }
#      # hostssd: {vms_by_az: {a: 1, b: 0, c: 0}, flavor: c3.large, auto_volumes: [{device_name: "/dev/sdb", mountpoint: "/var/log/mysvc", fstype: "ext4", "volume_type": "gp2", "volume_size": 2, ephemeral: False, encrypted: True, "delete_on_termination": true}]}
#      # hosthdd: {vms_by_az: {a: 1, b: 0, c: 0}, flavor: h1.2xlarge, auto_volumes: [{device_name: "/dev/sdb", mountpoint: "/var/log/mysvc", fstype: "ext4", "volume_type": "gp2", "volume_size": 2, ephemeral: False, encrypted: True, "delete_on_termination": true}]}
#    aws_access_key: ""
#    aws_secret_key: ""
#    vpc_name: "test{{buildenv}}"
#    vpc_subnet_name_prefix: "{{buildenv}}-test-{{_region}}"
#    key_name: "test__id_rsa"
#    termination_protection: "no"
#_cloud_type: *cloud_type
#_region: *region
#_dns_nameserver_zone: *dns_nameserver_zone

### GCP example
#cluster_vars:
#  type: &cloud_type "gcp"
#  image: "projects/ubuntu-os-cloud/global/images/ubuntu-2004-focal-v20200917"    # Ubuntu images can be located at https://cloud-images.ubuntu.com/locator/
#  image: "projects/ubuntu-os-cloud/global/images/ubuntu-1804-bionic-v20200923"   # Ubuntu images can be located at https://cloud-images.ubuntu.com/locator/
#  region: &region "europe-west1"
#  dns_cloud_internal_domain: "c.{{gcp_credentials_json.project_id}}.internal"    # The cloud-internal zone as defined by the cloud provider (e.g. GCP, AWS)
#  dns_nameserver_zone: &dns_nameserver_zone ""                                   # The zone that dns_server will operate on.  gcloud dns needs a trailing '.'.  Leave blank if no external DNS (use IPs only)
#  dns_user_domain: "{%- if _dns_nameserver_zone -%}CUSTOM.PREFIXES.{{_dns_nameserver_zone}}{%- endif -%}"         # A user-defined _domain_ part of the FDQN, (if more prefixes are required before the dns_nameserver_zone)
#  dns_server: ""                            # Specify DNS server. nsupdate, route53 or clouddns.  If empty string is specified, no DNS will be added.
#  assign_public_ip: "yes"
#  inventory_ip: "public"                    # 'public' or 'private', (private in case we're operating in a private LAN).  If public, 'assign_public_ip' must be 'yes'
#  ip_forward: "false"
#  ssh_guard_whitelist: &ssh_guard_whitelist ['10.0.0.0/8']    # Put your public-facing IPs into this (if you're going to access it via public IP), to avoid rate-limiting.
#  custom_tagslabels: {inv_resident_id: "abc", inv_proposition_id: "def"}
#  network_fw_tags: ["{{cluster_name}}-nwtag"]
#  firewall_rules:
#    - name: "{{cluster_name}}-extssh"
#      allowed: [{ip_protocol: "tcp", ports: ["22"]}]
#      source_ranges: "{{_ssh_guard_whitelist}}"
#      description: "SSH Access"
#    - name: "{{cluster_name}}-prometheus-node-exporter"
#      allowed: [{ip_protocol: "tcp", ports: ["{{ prometheus_node_exporter_port | default(9100) }}"]}]
#      source_tags: ["{{cluster_name}}-nwtag"]
#      description: "Prometheus instances attached to {{cluster_name}}-nwtag can access the exporter port(s)."
#    - name: "{{cluster_name}}-nwtag"
#      allowed: [{ip_protocol: "all"}]
#      source_tags: ["{{cluster_name}}-nwtag"]
#      description: "Access from all VMs attached to the {{cluster_name}}-nwtag group"
#  sandbox:
#    hosttype_vars:
#      sys: {vms_by_az: {b: 1, c: 1, d: 1}, flavor: f1-micro, rootvol_size: "10", version: "{{sys_version | default('')}}", auto_volumes: []}
#      #sysdisks: {vms_by_az: {b: 1, c: 1, d: 1}, flavor: f1-micro, rootvol_size: "10", version: "{{sysdisks_version | default('')}}", auto_volumes: [{auto_delete: true, interface: "SCSI", volume_size: 2, mountpoint: "/var/log/mysvc", fstype: "ext4", perms: {owner: "root", group: "sudo", mode: "775"}}, {auto_delete: true, interface: "SCSI", volume_size: 2,  mountpoint: "/var/log/mysvc2", fstype: "ext4"}, {auto_delete: true, interface: "SCSI", volume_size: 3,  mountpoint: "/var/log/mysvc3", fstype: "ext4"}]}
#    vpc_project_id: "{{gcp_credentials_json.project_id}}"             # AKA the 'service project' if Shared VPC (https://cloud.google.com/vpc/docs/shared-vpc) is in use.
#    vpc_host_project_id: "{{gcp_credentials_json.project_id}}"        # Would differ from vpc_project_id if Shared VPC is in use, (the networking is in a separate project)
#    vpc_network_name: "test-{{buildenv}}"
#    vpc_subnet_name: ""
#    preemptible: "no"
#    deletion_protection: "no"
#_cloud_type: *cloud_type
#_region: *region
#_ssh_guard_whitelist: *ssh_guard_whitelist
#_dns_nameserver_zone: *dns_nameserver_zone
