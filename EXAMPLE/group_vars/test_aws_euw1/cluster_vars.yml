---

redeploy_schemes_supported: ['_scheme_addallnew_rmdisk_rollback', '_scheme_addnewvm_rmdisk_rollback', '_scheme_rmvm_rmdisk_only', '_scheme_rmvm_keepdisk_rollback']

#redeploy_scheme: _scheme_addallnew_rmdisk_rollback
#redeploy_scheme: _scheme_addnewvm_rmdisk_rollback
#redeploy_scheme: _scheme_rmvm_rmdisk_only
#redeploy_scheme: _scheme_rmvm_keepdisk_rollback

app_name: "test"                  # The name of the application cluster (e.g. 'couchbase', 'nginx'); becomes part of cluster_name.
app_class: "test"                 # The class of application (e.g. 'database', 'webserver'); becomes part of the fqdn

beats_config:
  filebeat:
#    output_logstash_hosts: ["localhost:5044"]   # The destination hosts for filebeat-gathered logs
#    extra_logs_paths: # The array is optional, if you need to add more paths or files to scrape for logs
#      - /var/log/myapp/*.log
  metricbeat:
#    output_logstash_hosts: ["localhost:5044"]   # The destination hosts for metricbeat-gathered metrics
#    diskio:                       # Diskio retrieves metrics for all disks partitions by default. When diskio.include_devices is defined, only look for defined partitions
#      include_devices: ["sda", "sdb", "nvme0n1", "nvme1n1", "nvme2n1"]     

## Vulnerability scanners - Tenable and/ or Qualys cloud agents:
cloud_agent:
#  tenable:
#    service: "nessusagent"
#    debpackage: ""
#    bin_path: "/opt/nessus_agent/sbin"
#    nessus_key_id: ""
#    nessus_group_id: ""
#    proxy: {host: "", port: ""}
#  qualys:
#    service: "qualys-cloud-agent"
#    github_release: ""
#    bin_name: ""
#    bin_path: "/usr/local/qualys/cloud-agent/bin"
#    config_path: "/etc/default/qualys-cloud-agent"
#    activation_id: ""
#    customer_id: ""
#    proxy: {host: "", port: ""}

## Bind configuration and credentials, per environment
bind9:
  sandbox: {server: "", key_name: "", key_secret: ""}

cluster_name: "{{app_name}}-{{buildenv}}"       # Identifies the cluster within the cloud environment

cluster_vars:
  type: &cloud_type "aws"
  image: "ami-0c4c42893066a139e"            # eu-west-1, 20.04, amd64, hvm-ssd, 20200924.  Ubuntu images can be located at https://cloud-images.ubuntu.com/locator/
#  image: "ami-06868ad5a3642e4d7"            # eu-west-1, 18.04, amd64, hvm-ssd, 20200923.  Ubuntu images can be located at https://cloud-images.ubuntu.com/locator/
  region: &region "eu-west-1"       # eu-west-1, us-west-2
  dns_cloud_internal_domain: "{{_region}}.compute.internal"   # The cloud-internal zone as defined by the cloud provider (e.g. GCP, AWS)
  dns_nameserver_zone: &dns_nameserver_zone ""                # The zone that dns_server will operate on.  gcloud dns needs a trailing '.'.  Leave blank if no external DNS (use IPs only)
  dns_user_domain: "{%- if _dns_nameserver_zone -%}{{_cloud_type}}-{{_region}}.{{app_class}}.{{buildenv}}.{{_dns_nameserver_zone}}{%- endif -%}"         # A user-defined _domain_ part of the FDQN, (if more prefixes are required before the dns_nameserver_zone)
  dns_server: ""                    # Specify DNS server. nsupdate, route53 or clouddns.  If empty string is specified, no DNS will be added.
  route53_private_zone: true        # Only used when cluster_vars.type == 'aws'. Defaults to true if not set.
  assign_public_ip: "yes"
  inventory_ip: "public"            # 'public' or 'private', (private in case we're operating in a private LAN).  If public, 'assign_public_ip' must be 'yes'
  instance_profile_name: ""
  custom_tagslabels:
    inv_resident_id: "myresident"
    inv_proposition_id: "myproposition"
    inv_environment_id: "{{buildenv}}"
    inv_service_id: "{{app_class}}"
    inv_cluster_id: "{{cluster_name}}"
    inv_cluster_type: "{{app_name}}"
    inv_cost_centre: "1234"
  secgroups_existing: []
  secgroup_new:
    - proto: "tcp"
      ports: ["22"]
      cidr_ip: "0.0.0.0/0"
      rule_desc: "SSH Access"
    - proto: "tcp"
      ports: ["{{ prometheus_node_exporter_port | default(9100) }}"]
      group_name: ["{{buildenv}}-private-sg"]
      rule_desc: "Prometheus instances attached to {{buildenv}}-private-sg can access the exporter port(s)."
    - proto: all
      group_name: ["{{cluster_name}}-sg"]
      rule_desc: "Access from all VMs attached to the {{ cluster_name }}-sg group"
  sandbox:
    hosttype_vars:
      sys: {vms_by_az: {a: 1, b: 1, c: 1}, flavor: t3a.nano, version: "{{sys_version | default('')}}", auto_volumes: []}
#      sysdisks2: {vms_by_az: {a: 1, b: 0, c: 0}, flavor: t3a.nano, version: "{{sysdisks_version | default('')}}", auto_volumes: [{"device_name": "/dev/sdf", mountpoint: "/media/mysvc", fstype: "ext4", "volume_type": "gp2", "volume_size": 1, encrypted: True, "delete_on_termination": true, perms: {owner: "root", group: "sudo", mode: "775"} }, {"device_name": "/dev/sdg", mountpoint: "/media/mysvc2", fstype: "ext4", "volume_type": "gp2", "volume_size": 1, encrypted: True, "delete_on_termination": true}]}
#      sysdisks3: {vms_by_az: {a: 1, b: 0, c: 0}, flavor: t3a.nano, version: "{{sysdisks_version | default('')}}", auto_volumes: [{"device_name": "/dev/sdf", mountpoint: "/media/mysvc", fstype: "ext4", "volume_type": "gp2", "volume_size": 1, encrypted: True, "delete_on_termination": true, perms: {owner: "root", group: "sudo", mode: "775"} }, {"device_name": "/dev/sdg", mountpoint: "/media/mysvc2", fstype: "ext4", "volume_type": "gp2", "volume_size": 1, encrypted: True, "delete_on_termination": true}, {"device_name": "/dev/sdh", mountpoint: "/media/mysvc3", fstype: "ext4", "volume_type": "gp2", "volume_size": 1, encrypted: True, "delete_on_termination": true}]}
#      sysdisks-snapshot: {vms_by_az: {a: 1, b: 1, c: 0}, flavor: t3a.nano, version: "{{sys_version | default('')}}", auto_volumes: [{"snapshot_tags": {"tag:backup_id": "57180566894481854905"}, "device_name": "/dev/sdf", mountpoint: "/media/data", fstype: "ext4", "volume_type": "gp2", "volume_size": 1, encrypted: True, "delete_on_termination": true }]}
#      hostnvme-multi: {vms_by_az: {a: 1, b: 0, c: 0}, flavor: i3en.2xlarge, version: "{{sys_version | default('')}}", auto_volumes: [{device_name: "/dev/sdb", mountpoint: "/media/mysvc", fstype: "ext4", "volume_type": "ephemeral", ephemeral: ephemeral0}, {device_name: "/dev/sdc", mountpoint: "/media/mysvc2", fstype: "ext4", "volume_type": "ephemeral", ephemeral: ephemeral1}, {"device_name": "/dev/sdf", mountpoint: "/media/mysvc8", fstype: "ext4", "volume_type": "gp2", "volume_size": 1, encrypted: True, "delete_on_termination": true }] }
#      hostnvme-lvm: {vms_by_az: {a: 1, b: 0, c: 0}, flavor: i3en.2xlarge, version: "{{sys_version | default('')}}", auto_volumes: [{device_name: "/dev/sdb", mountpoint: "/media/data", fstype: "ext4", "volume_type": "ephemeral", ephemeral: ephemeral0}, {device_name: "/dev/sdc", mountpoint: "/media/data", fstype: "ext4", "volume_type": "ephemeral", ephemeral: ephemeral1}], lvmparams: {vg_name: "vg0", lv_name: "lv0", lv_size: "+100%FREE"} }
#      hosthdd-multi: {vms_by_az: {a: 1, b: 0, c: 0}, flavor: d2.xlarge, version: "{{sys_version | default('')}}", auto_volumes: [{device_name: "/dev/sdb", mountpoint: "/media/mysvc", fstype: "ext4", "volume_type": "ephemeral", ephemeral: ephemeral0}, {device_name: "/dev/sdc", mountpoint: "/media/mysvc2", fstype: "ext4", "volume_type": "ephemeral", ephemeral: ephemeral1}, {device_name: "/dev/sdd", mountpoint: "/media/mysvc3", fstype: "ext4", "volume_type": "ephemeral", ephemeral: ephemeral2}] }
#      hosthdd-lvm:   {vms_by_az: {a: 1, b: 0, c: 0}, flavor: d2.xlarge, version: "{{sys_version | default('')}}", auto_volumes: [{device_name: "/dev/sdb", mountpoint: "/media/data", fstype: "ext4", "volume_type": "ephemeral", ephemeral: ephemeral0}, {device_name: "/dev/sdc", mountpoint: "/media/data", fstype: "ext4", "volume_type": "ephemeral", ephemeral: ephemeral1}, {device_name: "/dev/sdd", mountpoint: "/media/data", fstype: "ext4", "volume_type": "ephemeral", ephemeral: ephemeral2}], lvmparams: {vg_name: "vg0", lv_name: "lv0", lv_size: "+100%FREE"} }
    aws_access_key: ""
    aws_secret_key: ""
    vpc_name: "test{{buildenv}}"
    vpc_subnet_name_prefix: "{{buildenv}}-test-{{_region}}"
    key_name: "test__id_rsa"
    termination_protection: "no"
_cloud_type: *cloud_type
_region: *region
_dns_nameserver_zone: *dns_nameserver_zone
