---

redeploy_scheme: _scheme_addallnew_rmdisk_rollback
#redeploy_scheme: _scheme_addnewvm_rmdisk_rollback
#redeploy_scheme: _scheme_rmvm_rmdisks_only

app_name: "test"                  # The name of the application cluster (e.g. 'couchbase', 'nginx'); becomes part of cluster_name.
app_class: "test"                 # The class of application (e.g. 'database', 'webserver'); becomes part of the fqdn

beats_target_hosts: []            # The destination hosts for e.g. filebeat/ metricbeat logs

## Vulnerability scanners - Tenable and/ or Qualys cloud agents:
cloud_agent:
#  tenable:
#    service: "nessusagent"
#    debpackage: ""
#    bin_path: "/opt/nessus_agent/sbin"
#    nessus_key_id: ""
#    nessus_group_id: ""
#    proxy: {host: "", port: ""}
#  qualys:
#    service: "qualys-cloud-agent"
#    debpackage: ""
#    bin_path: "/usr/local/qualys/cloud-agent/bin"
#    config_path: "/etc/default/qualys-cloud-agent"
#    activation_id: ""
#    customer_id: ""
#    proxy: {host: "", port: ""}

## Bind configuration and credentials, per environment
bind9:
  sandbox:
    server:
    key_name:
    key_secret:

cluster_name: "{{app_name}}-{{buildenv}}"       # Identifies the cluster within the cloud environment

cluster_vars:
  type: &cloud_type "aws"
  image: "ami-0964eb2dc8b836eb6"    # eu-west-1, 18.04, amd64, hvm-ssd, 20200430.  Ubuntu images can be located at https://cloud-images.ubuntu.com/locator/
  region: &region "eu-west-1"       # eu-west-1, us-west-2
  dns_cloud_internal_domain: "{{_region}}.compute.internal"   # The cloud-internal zone as defined by the cloud provider (e.g. GCP, AWS)
  dns_nameserver_zone: &dns_nameserver_zone ""                # The zone that dns_server will operate on.  gcloud dns needs a trailing '.'.  Leave blank if no external DNS (use IPs only)
  dns_user_domain: "{%- if _dns_nameserver_zone -%}{{_cloud_type}}-{{_region}}.{{app_class}}.{{buildenv}}.{{_dns_nameserver_zone}}{%- endif -%}"         # A user-defined _domain_ part of the FDQN, (if more prefixes are required before the dns_nameserver_zone)
  dns_server: ""                    # Specify DNS server. nsupdate, route53 or clouddns.  If empty string is specified, no DNS will be added.
  route53_private_zone: true        # Only used when cluster_vars.type == 'aws'. Defaults to true if not set.
  assign_public_ip: "yes"
  inventory_ip: "public"            # 'public' or 'private', (private in case we're operating in a private LAN).  If public, 'assign_public_ip' must be 'yes'
  instance_profile_name: ""
  custom_tagslabels:
    inv_resident_id: "myresident"
    inv_proposition_id: "myproposition"
    inv_environment_id: "{{buildenv}}"
    inv_service_id: "{{app_class}}"
    inv_cluster_id: "{{cluster_name}}"
    inv_cluster_type: "{{app_name}}"
    inv_cost_centre: "1234"
  secgroups_existing: []
  secgroup_new:
    - proto: "tcp"
      ports: ["22"]
      cidr_ip: "0.0.0.0/0"
      rule_desc: "SSH Access"
    - proto: "tcp"
      ports: ["{{ prometheus_node_exporter_port | default(9100) }}"]
      group_name: ["{{buildenv}}-private-sg"]
      rule_desc: "Prometheus instances attached to {{buildenv}}-private-sg can access the exporter port(s)."
    - proto: all
      group_name: ["{{cluster_name}}-sg"]
      rule_desc: "Access from all VMs attached to the {{ cluster_name }}-sg group"
  sandbox:
    hosttype_vars:
      sys: {vms_by_az: {a: 1, b: 1, c: 1}, flavor: t3a.nano, version: "{{sys_version | default('')}}", auto_volumes: []}
#      sysdisks: {vms_by_az: {a: 1, b: 0, c: 0}, flavor: t3a.nano, version: "{{sysdisks_version | default('')}}", auto_volumes: [{"device_name": "/dev/sdb", mountpoint: "/var/log/mysvc", fstype: "ext4", "volume_type": "gp2", "volume_size": 2, ephemeral: False, encrypted: True, "delete_on_termination": true, perms: {owner: "root", group: "sudo", mode: "775"} }, {"device_name": "/dev/sdc", mountpoint: "/var/log/mysvc2", fstype: "ext4", "volume_type": "gp2", "volume_size": 3, ephemeral: False, encrypted: True, "delete_on_termination": true}, {"device_name": "/dev/sdd", mountpoint: "/var/log/mysvc3", fstype: "ext4", "volume_type": "gp2", "volume_size": 2, ephemeral: False, encrypted: True, "delete_on_termination": true}]}
#      sysdisks_snapshot: {vms_by_az: {a: 1, b: 1, c: 0}, flavor: t3a.nano, auto_volumes: [{"snapshot_tags": {"tag:backup_id": "57180566894481854905"}, "device_name": "/dev/sdb", mountpoint: "/data", fstype: "ext4", "volume_type": "gp2", "volume_size": 2, ephemeral: False, encrypted: True, "delete_on_termination": true }]}
#      hostnvme_multi: {vms_by_az: {a: 1, b: 0, c: 0}, flavor: i3en.2xlarge, auto_volumes: [], nvme: {volumes: [{mountpoint: "/var/log/mysvc", fstype: ext4, volume_size: 2500}, {mountpoint: "/var/log/mysvc2", fstype: ext4, volume_size: 2500}]} }
#      hostnvme_lvm: {vms_by_az: {a: 1, b: 0, c: 0}, flavor: i3en.2xlarge, auto_volumes: [], nvme: {volumes: [{mountpoint: "/var/log/mysvc", fstype: ext4, volume_size: 2500}, {mountpoint: "/var/log/mysvc", fstype: ext4, volume_size: 2500}], lvmparams: {vg_name: "vg0", lv_name: "lv0", lv_size: "+100%FREE"} } }
#      hostssd: {vms_by_az: {a: 1, b: 0, c: 0}, flavor: c3.large, auto_volumes: [{device_name: "/dev/sdb", mountpoint: "/var/log/mysvc", fstype: "ext4", "volume_type": "gp2", "volume_size": 2, ephemeral: False, encrypted: True, "delete_on_termination": true}]}
#      hosthdd: {vms_by_az: {a: 1, b: 0, c: 0}, flavor: h1.2xlarge, auto_volumes: [{device_name: "/dev/sdb", mountpoint: "/var/log/mysvc", fstype: "ext4", "volume_type": "gp2", "volume_size": 2, ephemeral: False, encrypted: True, "delete_on_termination": true}]}
    aws_access_key: ""
    aws_secret_key: ""
    vpc_name: "test{{buildenv}}"
    vpc_subnet_name_prefix: "{{buildenv}}-test-{{_region}}"
    key_name: "test__id_rsa"
    termination_protection: "no"
_cloud_type: *cloud_type
_region: *region
_dns_nameserver_zone: *dns_nameserver_zone
