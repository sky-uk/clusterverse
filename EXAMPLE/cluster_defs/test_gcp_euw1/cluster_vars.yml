---

redeploy_schemes_supported: ['_scheme_addallnew_rmdisk_rollback', '_scheme_addnewvm_rmdisk_rollback', '_scheme_rmvm_rmdisk_only', '_scheme_rmvm_keepdisk_rollback']

#redeploy_scheme: _scheme_addallnew_rmdisk_rollback
#redeploy_scheme: _scheme_addnewvm_rmdisk_rollback
#redeploy_scheme: _scheme_rmvm_rmdisk_only
#redeploy_scheme: _scheme_rmvm_keepdisk_rollback

skip_dynamic_inventory_sshwait: true

prometheus_node_exporter_install: false
filebeat_install: false
metricbeat_install: false
canary: none

app_name: "test"                  # The name of the application cluster (e.g. 'couchbase', 'nginx'); becomes part of cluster_name.
app_class: "test"                 # The class of application (e.g. 'database', 'webserver'); becomes part of the fqdn

beats_config:
  filebeat:
#    output_logstash_hosts: ["localhost:5044"]        # The destination hosts for filebeat-gathered logs
#    extra_logs_paths:                                # The array is optional, if you need to add more paths or files to scrape for logs
#      - /var/log/myapp/*.log
  metricbeat:
#    output_logstash_hosts: ["localhost:5044"]        # The destination hosts for metricbeat-gathered metrics
#    diskio:                                          # Diskio retrieves metrics for all disks partitions by default. When diskio.include_devices is defined, only look for defined partitions
#      include_devices: ["sda", "sdb", "nvme0n1", "nvme1n1", "nvme2n1"]

## Vulnerability scanners - Tenable and/ or Qualys cloud agents:
cloud_agent:
#  tenable:
#    service: "nessusagent"
#    debpackage: ""
#    bin_path: "/opt/nessus_agent/sbin"
#    nessus_key_id: ""
#    nessus_group_id: ""
#    proxy: {host: "", port: ""}
#  qualys:
#    service: "qualys-cloud-agent"
#    github_release: ""
#    bin_name: ""
#    bin_path: "/usr/local/qualys/cloud-agent/bin"
#    config_path: "/etc/default/qualys-cloud-agent"
#    activation_id: ""
#    customer_id: ""
#    proxy: {host: "", port: ""}

## Bind configuration and credentials, per environment
bind9:
  sandbox: {server: "", key_name: "", key_secret: ""}

cluster_name: "{{app_name}}-{{buildenv}}"       # Identifies the cluster within the cloud environment

cluster_vars:
  type: &cloud_type "gcp"
  image: "projects/ubuntu-os-cloud/global/images/ubuntu-2004-focal-v20201211"           # Ubuntu images can be located at https://cloud-images.ubuntu.com/locator/
#  image: "projects/ubuntu-os-cloud/global/images/ubuntu-1804-bionic-v20201211a"
  region: &region "europe-west1"
  dns_cloud_internal_domain: "c.{{ (_gcp_service_account_rawtext | string | from_json).project_id }}.internal"         # The cloud-internal zone as defined by the cloud provider (e.g. GCP, AWS)
  dns_nameserver_zone: &dns_nameserver_zone ""                                                    # The zone that dns_server will operate on.  gcloud dns needs a trailing '.'.  Leave blank if no external DNS (use IPs only)
  dns_user_domain: "{%- if _dns_nameserver_zone -%}{{_cloud_type}}-{{_region}}.{{app_class}}.{{buildenv}}.{{_dns_nameserver_zone}}{%- endif -%}"         # A user-defined _domain_ part of the FDQN, (if more prefixes are required before the dns_nameserver_zone)
  dns_server: "clouddns"                                                                # Specify DNS server. nsupdate, route53 or clouddns.  If empty string is specified, no DNS will be added.
  assign_public_ip: "yes"
  inventory_ip: "public"                                                                # 'public' or 'private', (private in case we're operating in a private LAN).  If public, 'assign_public_ip' must be 'yes'
  ip_forward: "false"
  ssh_whitelist: &ssh_whitelist ['10.0.0.0/8']
  metadata:
    ssh-keys: "{{ cliargs.remote_user }}:{{ lookup('pipe', 'ssh-keygen -y -f ' + ansible_ssh_private_key_file) }} {{ cliargs.remote_user }}"
    startup-script: "{%- if _ssh_whitelist is defined and _ssh_whitelist | length > 0 -%}#! /bin/bash\n\n#Whitelist my inbound IPs\n[ -f /etc/sshguard/whitelist ] && echo \"{{_ssh_whitelist | join ('\n')}}\" >>/etc/sshguard/whitelist && /bin/systemctl restart sshguard{%- endif -%}"
    user-data: ""
  custom_tagslabels:
    inv_resident_id: "myresident"
    inv_proposition_id: "myproposition"
    inv_environment_id: "{{buildenv}}"
    inv_service_id: "{{app_class}}"
    inv_cluster_id: "{{cluster_name}}"
    inv_cluster_type: "{{app_name}}"
    inv_cost_centre: "1234"
  network_fw_tags: ["{{cluster_name}}-nwtag"]
  firewall_rules:
    - name: "{{cluster_name}}-extssh"
      allowed: [{ip_protocol: "tcp", ports: ["22"]}]
      source_ranges: "{{_ssh_whitelist}}"
      description: "SSH Access"
#    - name: "{{cluster_name}}-prometheus-node-exporter"
#      allowed: [{ip_protocol: "tcp", ports: ["{{ prometheus_node_exporter_port | default(9100) }}"]}]
#      source_tags: ["{{cluster_name}}-nwtag"]
#      description: "Prometheus instances attached to {{cluster_name}}-nwtag can access the exporter port(s)."
  sandbox:
    hosttype_vars:
      sys: {vms_by_az: {d: 1, b: 1, c: 0}, flavor: e2-micro, rootvol_size: "10", version: "{{sys_version | default('')}}", auto_volumes: []}
#      sysdisks: {vms_by_az: {d: 1, b: 1, c: 0}, flavor: e2-micro, rootvol_size: "10", version: "{{sysdisks_version | default('')}}", auto_volumes: [{auto_delete: true, interface: "SCSI", volume_size: 2, mountpoint: "/media/mysvc", fstype: "ext4"}]}
      sysdisks2: {vms_by_az: {d: 1, b: 1, c: 0}, flavor: e2-micro, rootvol_size: "10", version: "{{sysdisks_version | default('')}}", auto_volumes: [{auto_delete: true, interface: "SCSI", volume_size: 2, mountpoint: "/media/mysvc", fstype: "ext4"}, {auto_delete: true, interface: "SCSI", volume_size: 2, mountpoint: "/media/mysvc2", fstype: "ext4"}]}
#      sysdisks_multi: {vms_by_az: {d: 1, b: 1, c: 0}, flavor: e2-micro, rootvol_size: "10", version: "{{sysdisks_version | default('')}}", auto_volumes: [{auto_delete: true, interface: "SCSI", volume_size: 2, mountpoint: "/var/log/mysvc", fstype: "ext4", perms: {owner: "root", group: "sudo", mode: "775"}}, {auto_delete: true, interface: "SCSI", volume_size: 2,  mountpoint: "/var/log/mysvc2", fstype: "ext4"}, {auto_delete: true, interface: "SCSI", volume_size: 3,  mountpoint: "/var/log/mysvc3", fstype: "ext4"}]}
    gcp_service_account_rawtext: &gcp_service_account_rawtext !vault |
      $ANSIBLE_VAULT;1.1;AES256
      7669080460651349243347331538721104778691266429457726036813912140404310
    ssh_connection_cfg:
      host:
        ansible_user: "ansible"
        ansible_ssh_private_key_file: !vault |
          $ANSIBLE_VAULT;1.2;AES256;mgmt
          7669080460651349243347331538721104778691266429457726036813912140404310
#      bastion:
#        ssh_args: '-o ProxyCommand="ssh -i ./id_rsa_bastion -W %h:%p -q top@10.119.113.133"'
#        ssh_priv_key: !vault |
#          $ANSIBLE_VAULT;1.2;AES256;mgmt
#          7669080460651349243347331538721104778691266429457726036813912140404310
    vpc_project_id: "{{ (_gcp_service_account_rawtext | string | from_json).project_id }}"             # AKA the 'service project' if Shared VPC (https://cloud.google.com/vpc/docs/shared-vpc) is in use.
    vpc_host_project_id: "{{ (_gcp_service_account_rawtext | string | from_json).project_id }}"        # Would differ from vpc_project_id if Shared VPC is in use, (the networking is in a separate project)
    vpc_network_name: "test-{{buildenv}}"
    vpc_subnet_name: ""
    preemptible: "no"
    deletion_protection: "no"
_cloud_type: *cloud_type
_region: *region
_gcp_service_account_rawtext: *gcp_service_account_rawtext
_ssh_whitelist: *ssh_whitelist
_dns_nameserver_zone: *dns_nameserver_zone
