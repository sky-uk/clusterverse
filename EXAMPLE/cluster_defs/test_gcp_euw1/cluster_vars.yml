---

redeploy_schemes_supported: ['_scheme_addallnew_rmdisk_rollback', '_scheme_addnewvm_rmdisk_rollback', '_scheme_rmvm_rmdisk_only', '_scheme_rmvm_keepdisk_rollback']

#redeploy_scheme: _scheme_addallnew_rmdisk_rollback
#redeploy_scheme: _scheme_addnewvm_rmdisk_rollback
#redeploy_scheme: _scheme_rmvm_rmdisk_only
#redeploy_scheme: _scheme_rmvm_keepdisk_rollback

skip_dynamic_inventory_sshwait: true

prometheus_node_exporter_install: false
filebeat_install: false
metricbeat_install: false

app_name: "{{lookup('pipe', 'whoami')}}-test"   # The name of the application cluster (e.g. 'couchbase', 'nginx'); becomes part of cluster_name.  Provided is a default to ensure no accidental overwriting.
app_class: "test"                               # The class of application (e.g. 'database', 'webserver'); becomes part of the fqdn

beats_config:
  filebeat:
#    output_logstash_hosts: ["localhost:5044"]        # The destination hosts for filebeat-gathered logs
#    extra_logs_paths:                                # The array is optional, if you need to add more paths or files to scrape for logs
#      - /var/log/myapp/*.log
  metricbeat:
#    output_logstash_hosts: ["localhost:5044"]        # The destination hosts for metricbeat-gathered metrics
#    diskio:                                          # Diskio retrieves metrics for all disks partitions by default. When diskio.include_devices is defined, only look for defined partitions
#      include_devices: ["sda", "sdb", "nvme0n1", "nvme1n1", "nvme2n1"]

## Vulnerability scanners - Tenable and/ or Qualys cloud agents:
cloud_agent:
#  tenable:
#    service: "nessusagent"
#    debpackage: ""
#    bin_path: "/opt/nessus_agent/sbin"
#    nessus_key_id: ""
#    nessus_group_id: ""
#    proxy: {host: "", port: ""}
#  qualys:
#    service: "qualys-cloud-agent"
#    github_release: ""
#    bin_name: ""
#    bin_path: "/usr/local/qualys/cloud-agent/bin"
#    config_path: "/etc/default/qualys-cloud-agent"
#    activation_id: ""
#    customer_id: ""
#    proxy: {host: "", port: ""}


cluster_name: "{{app_name}}-{{buildenv}}"       # Identifies the cluster within the cloud environment

cluster_vars:
  type: &cloud_type "gcp"
  image: "projects/ubuntu-os-cloud/global/images/ubuntu-2004-focal-*"      # Latest Ubuntu Focal (20.04.x) image
  #image: "projects/centos-cloud/global/images/centos-7-*"                  # Latest CentOS 7.x image
  #image: "projects/almalinux-cloud/global/images/almalinux-8-*"            # Latest AlmaLinux 8.x OS image
  #image: "projects/ubuntu-os-cloud/global/images/ubuntu-2004-focal-v20210702"
  #image: "projects/centos-cloud/global/images/centos-7-v20210701"
  #image: "projects/almalinux-cloud/global/images/almalinux-8-v20210701"
  region: &region "europe-west4"
  dns_cloud_internal_domain: "c.{{ (_gcp_service_account_rawtext | string | from_json).project_id }}.internal"         # The cloud-internal zone as defined by the cloud provider (e.g. GCP, AWS)
  dns_nameserver_zone: &dns_nameserver_zone ""                                                    # The zone that dns_server will operate on.  gcloud dns needs a trailing '.'.  Leave blank if no external DNS (use IPs only)
  dns_user_domain: "{%- if _dns_nameserver_zone -%}{{_cloud_type}}-{{_region}}.{{app_class}}.{{buildenv}}.{{_dns_nameserver_zone}}{%- endif -%}"         # A user-defined _domain_ part of the FDQN, (if more prefixes are required before the dns_nameserver_zone)
  dns_server: "clouddns"                                                                          # Specify DNS server. nsupdate, route53 or clouddns.  If empty string is specified, no DNS will be added.
  assign_public_ip: "no"
  inventory_ip: "private"                                                                         # 'public' or 'private', (private in case we're operating in a private LAN).  If public, 'assign_public_ip' must be 'yes'
  ip_forward: "false"
  ssh_whitelist: &ssh_whitelist ['10.0.0.0/8']
  metadata:
    #The ssh key is either provided on the command line (as 'ansible_ssh_private_key_file'), or as a variable in cluster_vars[buildenv].ssh_connection_cfg.host.ansible_ssh_private_key_file (anchored to _host_ssh_connection_cfg.ansible_ssh_private_key_file); we can slurp the key from either variable, and then ssh-keygen it into the public key (we have to remove the comment though before we add our own, (hence the regex), because this is what gcp expects).
    ssh-keys: "{%- if _host_ssh_connection_cfg.ansible_ssh_private_key_file is defined -%}{{ _host_ssh_connection_cfg.ansible_user }}:{{ lookup('pipe', 'ssh-keygen -y -f /dev/stdin <<SSHFILE\n' + _host_ssh_connection_cfg.ansible_ssh_private_key_file|string + '\nSSHFILE') | regex_replace('([\\S]+ [\\S]+)(?:.*$)?', '\\1') }} {{ _host_ssh_connection_cfg.ansible_user }}{%- else -%}{{ cliargs.remote_user }}:{{ lookup('pipe', 'ssh-keygen -y -f ' + ansible_ssh_private_key_file) | regex_replace('([\\S]+ [\\S]+)(?:.*$)?', '\\1') }} {{ cliargs.remote_user }}{%- endif -%}"
    startup-script: "{%- if _ssh_whitelist is defined and _ssh_whitelist | length > 0 -%}#! /bin/bash\n\n#Whitelist my inbound IPs\n[ -f /etc/sshguard/whitelist ] && echo \"{{_ssh_whitelist | join ('\n')}}\" >>/etc/sshguard/whitelist && /bin/systemctl restart sshguard{%- endif -%}"
    user-data: ""
  custom_tagslabels:
    inv_resident_id: "myresident"
    inv_proposition_id: "myproposition"
    inv_cost_centre: "0000000000"
    inv_environment_id: "{{buildenv}}"
    inv_service_id: "{{app_class}}"
    inv_cluster_id: "{{cluster_name}}"
    inv_cluster_type: "{{app_name}}"
  network_fw_tags: ["{{cluster_name}}-nwtag"]
  firewall_rules:
    - name: "{{cluster_name}}-extssh"
      allowed: [{ip_protocol: "tcp", ports: ["22"]}]
      source_ranges: "{{_ssh_whitelist}}"
      description: "SSH Access"
    - name: "{{cluster_name}}-nwtag"
      allowed: [{ip_protocol: "all"}]
      source_tags: ["{{cluster_name}}-nwtag"]
      description: "Access from all VMs attached to the {{cluster_name}}-nwtag group"
    - name: "{{cluster_name}}-prometheus-node-exporter"
      allowed: [{ip_protocol: "tcp", ports: ["{{ prometheus_node_exporter_port | default(9100) }}"]}]
      source_tags: ["{{cluster_name}}-nwtag"]
      description: "Prometheus instances attached to {{cluster_name}}-nwtag can access the exporter port(s)."
  sandbox:
    hosttype_vars:
      sys: {vms_by_az: {a: 1, b: 1, c: 0}, flavor: e2-micro, version: '{{sys_version | default('''')}}', auto_volumes: []}
      sysdisks2: {vms_by_az: {a: 1, b: 1, c: 0}, flavor: e2-micro, version: '{{sysdisks_version | default('''')}}', rootvol_size: '25', auto_volumes: [{auto_delete: true, interface: SCSI, volume_size: 1, mountpoint: /media/mysvc, fstype: ext4, perms: {owner: root, group: root, mode: '775'}}, {auto_delete: true, interface: SCSI, volume_size: 1, mountpoint: /media/mysvc2, fstype: ext4}]}
      sysdisks3: {vms_by_az: {a: 1, b: 1, c: 0}, flavor: e2-micro, version: '{{sysdisks_version | default('''')}}', auto_volumes: [{auto_delete: true, interface: SCSI, volume_size: 1, mountpoint: /media/mysvc, fstype: ext4}, {auto_delete: true, interface: SCSI, volume_size: 1, mountpoint: /media/mysvc2, fstype: ext4}, {auto_delete: true, interface: SCSI, volume_size: 3, mountpoint: /media/mysvc3, fstype: ext4}]}
    gcp_service_account_rawtext: &gcp_service_account_rawtext !vault |
      $ANSIBLE_VAULT;1.2;AES256;sandbox
      7669080460651349243347331538721104778691266429457726036813912140404310
    ssh_connection_cfg:
      host: &host_ssh_connection_cfg
        ansible_user: "ansible"
        ansible_ssh_private_key_file: !vault |
          $ANSIBLE_VAULT;1.2;AES256;sandbox
          7669080460651349243347331538721104778691266429457726036813912140404310
      bastion:
        ssh_args: '-o ProxyCommand="ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i ./id_rsa_bastion -W %h:%p -q user@192.168.0.1"'
        ssh_priv_key: !vault |
          $ANSIBLE_VAULT;1.2;AES256;sandbox
          7669080460651349243347331538721104778691266429457726036813912140404310
    vpc_project_id: "{{ (_gcp_service_account_rawtext | string | from_json).project_id }}"            # AKA the 'service project' if Shared VPC (https://cloud.google.com/vpc/docs/shared-vpc) is in use.
    vpc_host_project_id: "{{ (_gcp_service_account_rawtext | string | from_json).project_id }}"       # Would differ from vpc_project_id if Shared VPC is in use, (the networking is in a separate project)
    vpc_network_name: "test-{{buildenv}}"
    vpc_subnet_name: ""                                                                               # Can be omitted if using default subnets
    preemptible: "no"
    deletion_protection: "no"
    nsupdate_cfg: {server: "", key_name: "", key_secret: ""}      # If you're using bind9 (or other nsupdate-compatible 'dns_server' (defined above))

_cloud_type: *cloud_type
_region: *region
_gcp_service_account_rawtext: *gcp_service_account_rawtext
_ssh_whitelist: *ssh_whitelist
_dns_nameserver_zone: *dns_nameserver_zone
_host_ssh_connection_cfg: { <<: *host_ssh_connection_cfg }
