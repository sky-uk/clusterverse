---

redeploy_schemes_supported: ['_scheme_addallnew_rmdisk_rollback', '_scheme_addnewvm_rmdisk_rollback', '_scheme_rmvm_rmdisk_only', '_scheme_rmvm_keepdisk_rollback']

#redeploy_scheme: _scheme_addallnew_rmdisk_rollback
#redeploy_scheme: _scheme_addnewvm_rmdisk_rollback
#redeploy_scheme: _scheme_rmvm_rmdisk_only
#redeploy_scheme: _scheme_rmvm_keepdisk_rollback

skip_dynamic_inventory_sshwait: true

prometheus_node_exporter_install: false
filebeat_install: false
metricbeat_install: false

app_name: "{{lookup('pipe', 'whoami')}}-test"   # The name of the application cluster (e.g. 'couchbase', 'nginx'); becomes part of cluster_name.  Provided is a default to ensure no accidental overwriting.
app_class: "test"                               # The class of application (e.g. 'database', 'webserver'); becomes part of the fqdn

beats_config:
  filebeat:
#    output_logstash_hosts: ["localhost:5044"]        # The destination hosts for filebeat-gathered logs
#    extra_logs_paths:                                # The array is optional, if you need to add more paths or files to scrape for logs
#      - /var/log/myapp/*.log
  metricbeat:
#    output_logstash_hosts: ["localhost:5044"]        # The destination hosts for metricbeat-gathered metrics
#    diskio:                                          # Diskio retrieves metrics for all disks partitions by default. When diskio.include_devices is defined, only look for defined partitions
#      include_devices: ["sda", "sdb", "nvme0n1", "nvme1n1", "nvme2n1"]

## Vulnerability scanners - Tenable and/ or Qualys cloud agents:
cloud_agent:
#  tenable:
#    service: "nessusagent"
#    debpackage: ""
#    bin_path: "/opt/nessus_agent/sbin"
#    nessus_key_id: ""
#    nessus_group_id: ""
#    proxy: {host: "", port: ""}
#  qualys:
#    service: "qualys-cloud-agent"
#    github_release: ""
#    bin_name: ""
#    bin_path: "/usr/local/qualys/cloud-agent/bin"
#    config_path: "/etc/default/qualys-cloud-agent"
#    activation_id: ""
#    customer_id: ""
#    proxy: {host: "", port: ""}


cluster_name: "{{app_name}}-{{buildenv}}"       # Identifies the cluster within the cloud environment

cluster_vars:
  type: &cloud_type "aws"
  # Either specify absolute image AMIs, or find the latest images based on location name filter.
  image: "099720109477/ubuntu/images/hvm-ssd/ubuntu-focal-20.04-*"     # Latest official Canonical Ubuntu Focal (20.04.x) image
  #image: "161831738826/centos-7-pke-*"                                 # Latest 'Banzai Cloud' CentOS 7.x image
  #image: "764336703387/AlmaLinux OS 8.*"                               # Latest official AlmaLinux 8.x OS image
  #image: "ami-03caf24deed650e2c"                   # eu-west-1 20.04 amd64 hvm-ssd 20210621.  Ubuntu images can be located at https://cloud-images.ubuntu.com/locator/
  #image: "ami-01f35c358a86763b2"                   # eu-west-1 CentOS 7 by Banzai Cloud
  #image: "ami-05d7345cebf7a784f"                   # eu-west-1 Official AlmaLinux 8.x OS image
  region: &region "eu-west-1"
  dns_cloud_internal_domain: "{{_region}}.compute.internal"       # The cloud-internal zone as defined by the cloud provider (e.g. GCP, AWS)
  dns_nameserver_zone: &dns_nameserver_zone ""                    # The zone that dns_server will operate on.  gcloud dns needs a trailing '.'.  Leave blank if no external DNS (use IPs only)
  dns_user_domain: "{%- if _dns_nameserver_zone -%}{{_cloud_type}}-{{_region}}.{{app_class}}.{{buildenv}}.{{_dns_nameserver_zone}}{%- endif -%}"         # A user-defined _domain_ part of the FDQN, (if more prefixes are required before the dns_nameserver_zone)
  dns_server: ""                              # Specify DNS server. nsupdate, route53 or clouddns.  If empty string is specified, no DNS will be added.
  route53_private_zone: yes                   # Only used when cluster_vars.type == 'aws'. Defaults to true if not set.
  assign_public_ip: "no"
  inventory_ip: "private"                     # 'public' or 'private', (private in case we're operating in a private LAN).  If public, 'assign_public_ip' must be 'yes'
  instance_profile_name: ""
  user_data: |-
    #cloud-config
    system_info:
      default_user:
        name: ansible
  custom_tagslabels:
    inv_resident_id: "myresident"
    inv_proposition_id: "myproposition"
    inv_cost_centre: "0000000000"
    inv_environment_id: "{{buildenv}}"
    inv_service_id: "{{app_class}}"
    inv_cluster_id: "{{cluster_name}}"
    inv_cluster_type: "{{app_name}}"
  ssh_whitelist: &ssh_whitelist ['10.0.0.0/8']
  secgroups_existing: []
  secgroup_new:
    - proto: "tcp"
      ports: ["22"]
      cidr_ip: "{{_ssh_whitelist}}"
      rule_desc: "SSH Access"
#    - proto: all
#      group_name: "{{cluster_name}}-sg"
#      rule_desc: "Access from all VMs attached to the {{ cluster_name }}-sg group"
#    - proto: "tcp"
#      ports: ["{{ prometheus_node_exporter_port | default(9100) }}"]
#      group_name: "{{buildenv}}-private-sg"
#      rule_desc: "Prometheus instances attached to {{buildenv}}-private-sg can access the exporter port(s)."
  sandbox:
    hosttype_vars:
      sys: {auto_volumes: [], flavor: t3a.nano, version: '{{sys_version | default('''')}}', vms_by_az: {a: 1, b: 1, c: 0}}
      sysdisks2: {auto_volumes: [{device_name: /dev/sda1, mountpoint: /, fstype: ext4, volume_type: gp2, volume_size: 9, encrypted: true, delete_on_termination: true}, {device_name: /dev/sdf, mountpoint: /media/mysvc, fstype: ext4, volume_type: gp2, volume_size: 1, encrypted: true, delete_on_termination: true, perms: {owner: root, group: root, mode: '775'}}, {device_name: /dev/sdg, mountpoint: /media/mysvc2, fstype: ext4, volume_type: gp2, volume_size: 1, iops: 100, encrypted: true, delete_on_termination: true}], flavor: t3a.nano, version: '{{sysdisks_version | default('''')}}', vms_by_az: {a: 1, b: 1, c: 0}}
      sysdisks3: {auto_volumes: [{device_name: /dev/sdf, mountpoint: /media/mysvc, fstype: ext4, volume_type: gp2, volume_size: 1, encrypted: true, delete_on_termination: true}, {device_name: /dev/sdg, mountpoint: /media/mysvc2, fstype: ext4, volume_type: gp2, volume_size: 1, encrypted: true, delete_on_termination: true}, {device_name: /dev/sdh, mountpoint: /media/mysvc3, fstype: ext4, volume_type: gp2, volume_size: 1, iops: 100, encrypted: true, delete_on_termination: true}], flavor: t3a.nano, version: '{{sysdisks_version | default('''')}}', vms_by_az: {a: 1, b: 1, c: 0}}
      hostnvme-multi: {auto_volumes: [{device_name: "/dev/sda1", mountpoint: "/", fstype: "ext4", volume_type: "gp2", volume_size: 8, encrypted: True, delete_on_termination: true}, {device_name: /dev/sdb, mountpoint: /media/mysvc, fstype: ext4, volume_type: ephemeral, ephemeral: ephemeral0}, {device_name: /dev/sdc, mountpoint: /media/mysvc2, fstype: ext4, volume_type: ephemeral, ephemeral: ephemeral1}, {device_name: /dev/sdf, mountpoint: /media/mysvc8, fstype: ext4, volume_type: gp2, volume_size: 1, encrypted: true, delete_on_termination: true}], flavor: i3en.2xlarge, version: '{{sys_version | default('''')}}', vms_by_az: {a: 1, b: 1, c: 0}}
      hostnvme-lvm: {auto_volumes: [{device_name: "/dev/sda1", mountpoint: "/", fstype: "ext4", volume_type: "gp2", volume_size: 8, encrypted: True, delete_on_termination: true}, {device_name: /dev/sdb, mountpoint: /media/data, fstype: ext4, volume_type: ephemeral, ephemeral: ephemeral0}, {device_name: /dev/sdc, mountpoint: /media/data, fstype: ext4, volume_type: ephemeral, ephemeral: ephemeral1}], lvmparams: {vg_name: vg0, lv_name: lv0, lv_size: 100%VG}, flavor: i3en.2xlarge, version: '{{sys_version | default('''')}}', vms_by_az: {a: 1, b: 1, c: 0}}
      hosthdd-multi: {auto_volumes: [{device_name: /dev/sdb, mountpoint: /media/mysvc, fstype: ext4, volume_type: ephemeral, ephemeral: ephemeral0}, {device_name: /dev/sdc, mountpoint: /media/mysvc2, fstype: ext4, volume_type: ephemeral, ephemeral: ephemeral1}, {device_name: /dev/sdd, mountpoint: /media/mysvc3, fstype: ext4, volume_type: ephemeral, ephemeral: ephemeral2}], flavor: d2.xlarge, version: '{{sys_version | default('''')}}', vms_by_az: {a: 1, b: 1, c: 0}}
      hosthdd-lvm: {auto_volumes: [{device_name: /dev/sdb, mountpoint: /media/data, fstype: ext4, volume_type: ephemeral, ephemeral: ephemeral0}, {device_name: /dev/sdc, mountpoint: /media/data, fstype: ext4, volume_type: ephemeral, ephemeral: ephemeral1}, {device_name: /dev/sdd, mountpoint: /media/data, fstype: ext4, volume_type: ephemeral, ephemeral: ephemeral2}], lvmparams: {vg_name: vg0, lv_name: lv0, lv_size: 100%VG}, flavor: d2.xlarge, version: '{{sys_version | default('''')}}', vms_by_az: {a: 1, b: 1, c: 0}}
    aws_access_key: !vault |
      $ANSIBLE_VAULT;1.2;AES256;sandbox
      7669080460651349243347331538721104778691266429457726036813912140404310
    aws_secret_key: !vault |
      $ANSIBLE_VAULT;1.2;AES256;sandbox
      7669080460651349243347331538721104778691266429457726036813912140404310
    ssh_connection_cfg:
      host: &host_ssh_connection_cfg
        ansible_user: "ansible"
        ansible_ssh_private_key_file: !vault |
          $ANSIBLE_VAULT;1.2;AES256;sandbox
          7669080460651349243347331538721104778691266429457726036813912140404310
      bastion:
        ssh_args: '-o ProxyCommand="ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i ./id_rsa_bastion -W %h:%p -q user@192.168.0.1"'
        ssh_priv_key: !vault |
          $ANSIBLE_VAULT;1.2;AES256;sandbox
          7669080460651349243347331538721104778691266429457726036813912140404310
    vpc_name: "test{{buildenv}}"
    vpc_subnet_name_prefix: "{{buildenv}}-test-{{_region}}"
    key_name: "test__id_rsa"
    termination_protection: "no"
    nsupdate_cfg: {server: "", key_name: "", key_secret: ""}      # If you're using bind9 (or other nsupdate-compatible 'dns_server' (defined above))

_cloud_type: *cloud_type
_region: *region
_ssh_whitelist: *ssh_whitelist
_dns_nameserver_zone: *dns_nameserver_zone
_host_ssh_connection_cfg: { <<: *host_ssh_connection_cfg }
