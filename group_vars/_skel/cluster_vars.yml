---

# Openstack and Google Cloud credentials lookup
os_username: "{{ lookup('env','OS_USERNAME') }}"
os_password: "{{ lookup('env','OS_PASSWORD') }}"
gcp_credentials_file: "{{ lookup('env','GCP_CREDENTIALS') | default('/dev/null', true) }}"
gcp_credentials_json: "{{ lookup('file', gcp_credentials_file) | default({'project_id': 'GCP_CREDENTIALS__NOT_SET','client_email': 'GCP_CREDENTIALS__NOT_SET'}, true) }}"

## Default user vars - in general to be provided by Jenkins
buildenv: ""                      # The environment (dev, stage, etc), which must be an attribute of cluster_vars
release_version: ""               # Identifies the application version that is being deployed

app_name: ""                      # The name of the application cluster (e.g. 'couchbase', 'nginx'); becomes part of cluster_name.
app_class: ""                     # The class of application (e.g. 'database', 'webserver'); becomes part of the fqdn

dns_tld_external: ""              # Top-level domain (above the level defined per clusterid)

## Vulnerablity scanners - Tenable and/ or Qualys cloud agents:
cloud_agent:
  tenable:
    service: "nessusagent"
    debpackage: ""
    bin_path: "/opt/nessus_agent/sbin"
    nessus_key_id: ""
    nessus_group_id: ""
    proxy:
      host: ""
      port: ""
  qualys:
    service: "qualys-cloud-agent"
    debpackage: ""
    bin_path: "/usr/local/qualys/cloud-agent/bin"
    config_path: "/etc/default/qualys-cloud-agent"
    activation_id: ""
    customer_id: ""
    proxy:
      host: ""
      port: ""

# Infoblox configurations and credentials
infoblox:
  sandbox:
    server: ""
    api_version: 2.6.1
    username: ""
    password: ""

# Bind configurations and credentials
bind9:
  sandbox:
    server: 
    key_name: 
    key_secret: 

# The global {{cluster_name}} is prefixed with {{app_name}}
cluster_name: "{{app_name}}-{{buildenv}}"

### AWS example
cluster_vars:
  type: aws     # aws, gcp, openstack
  image: ""     # Ubuntu images can be located at https://cloud-images.ubuntu.com/locator/
#  image: "ami-0987ee37af7792903"    #eu-west-1	xenial	16.04 LTS	amd64	hvm:ebs-ssd	20191114
#  image: "ami-04c58523038d79132"    #eu-west-1	bionic	18.04 LTS	amd64	hvm:ebs-ssd	20191113
#  image: "ami-0bbe9b07c5fe8e86e"    #us-west-2	xenial	16.04 LTS	amd64	hvm:ebs-ssd	20191114
#  image: "ami-0a7d051a1c4b54f65"    #us-west-2	bionic	18.04 LTS	amd64	hvm:ebs-ssd	20191113
  region: &region "eu-west-1"
  dns_zone_internal: "{{_region}}.compute.internal" # eu-west-1, us-west-2
  dns_zone_external: "{%- if dns_tld_external -%} <cloud+region e.g. aws_euw1, gce_euw1>.{{app_class}}.{{buildenv}}.{{dns_tld_external}} {%- endif -%}"
  dns_server: ""    # Specify DNS server. nsupdate, route53, infoblox or openstack (on openstack) supported.  If empty string is specified, no DNS will be added.
  assign_public_ip: "no"
  inventory_ip: "private"   # 'public' or 'private', (private in case we're operating in a private LAN).  If public, 'assign_public_ip' must be 'yes'
  instance_profile_name: "vpc_lock_{{buildenv}}"
  route53_private_zone: true # Only used when cluster_vars.type == 'aws'. Defaults to true if not set.
  secgroups_existing: []
  secgroup_new:
    - proto: "tcp"
      ports: ["22"]
      cidr_ip: 10.0.0.0/8
      rule_desc: "SSH Access"
    - proto: "tcp"
      ports: ["{{prometheus_node_exporter_port}}"]
      group_name: ["{{buildenv}}-private-sg"]
      rule_desc: "Prometheus instances attached to {{buildenv}}-private-sg can access the exporter port(s)."
    - proto: all
      group_name: ["{{cluster_name}}-sg"]
      rule_desc: "Access from all VMs attached to the {{ cluster_name }}-sg group"
  sandbox:
    hosttype_vars:
      sys: {count_per_az: 1, az: ["a", "b"], flavor: t3.micro, auto_volumes: []}
      #sysdisks: {count_per_az: 1, az: ["a", "b"], flavor: t3.micro, auto_volumes: [{"device_name": "/dev/sdb", mountpoint: "/var/log", "volume_type": "gp2", "volume_size": 2, ephemeral: False, encrypted: True, "delete_on_termination": true}, {"device_name": "/dev/sdc", mountpoint: "/mnt/test2", "volume_type": "gp2", "volume_size": 2, ephemeral: False, encrypted: True, "delete_on_termination": true}, {"device_name": "/dev/sdd", mountpoint: "/mnt/test3", "volume_type": "gp2", "volume_size": 2, ephemeral: False, encrypted: True, "delete_on_termination": true}]}
    aws_access_key: ""
    aws_secret_key: ""
    vpc_name: "<team_name>-{{buildenv}}"
    vpc_subnet_name_prefix: "{{buildenv}}-private-subnet-{{_region}}"
    key_name: ""
    termination_protection: "no"
_region: *region

## OPTIONAL - NVME/Nitro instances Logical Volumes configuration in case the mounting point for all nvme_volumes is set to the same path
vg_name: vg0
lv_name: lv0
lv_size: 100%FREE


### GCE example
#cluster_vars:
#  type: gce
#  image: "projects/ubuntu-os-cloud/global/images/ubuntu-1804-bionic-v20191113"
#  region: &region "europe-west1"
#  dns_zone_internal: "c.{{gcp_credentials_json.project_id}}.internal"
#  dns_zone_external: "{%- if dns_tld_external -%} <cloud+region e.g. gce_euw1>.{{app_class}}.{{buildenv}}.{{dns_tld_external}} {%- endif -%}"
#  dns_server: ""    # Specify DNS server. nsupdate, route53, infoblox or openstack (on openstack) supported.  If empty string is specified, no DNS will be added.
#  assign_public_ip: "yes"
#  inventory_ip: "public"     # 'public' or 'private', (private in case we're operating in a private LAN).  If public, 'assign_public_ip' must be 'yes'
#  project_id: "{{gcp_credentials_json.project_id}}"
#  ip_forward: "false"
#  network_fw_tags: ["{{cluster_name}}-nwtag"]
#  firewall_rules:
#    - name: "{{cluster_name}}-extssh"
#      allowed: [{ip_protocol: "tcp", ports: ["22"]}]
#      source_ranges: ['10.0.0.0/8']
#      description: "SSH Access"
#    - name: "{{cluster_name}}-nwtag"
#      allowed: [{ip_protocol: "all"}]
#      source_tags: ["{{cluster_name}}-nwtag"]
#      description: "Access from all VMs attached to the {{cluster_name}}-nwtag group"
#  sandbox:
#    hosttype_vars:
#      data: {count_per_az: 1, az: ["b"], flavor: f1-micro, rootvol_size: "10", auto_volumes: [{auto_delete: true, interface: "SCSI", volume_size: 2, mountpoint: "/mnt/data", fstype: "ext4"}, {auto_delete: true, interface: "SCSI", volume_size: 3,  mountpoint: "/mnt/data2", fstype: "ext4"}, {auto_delete: true, interface: "SCSI", volume_size: 2,  mountpoint: "/mnt/data3", fstype: "ext4"}]}
#    #  query: {count_per_az: 1, az: ["b", "c"], flavor: f1-micro, rootvol_size: "10", auto_volumes: []}
#    vpc_network_name: "<team_name>-{{buildenv}}"
#    vpc_subnet_name: ""
#    preemptible: "false"
#    deletion_protection: "no"
#_region: *region

### Openstack example
#cluster_vars:
#  type: openstack
#  image: "centos7"    #OR "CentOS-7-cloudimg-x64-20170612",  "Ubuntu-1604-Xenial-cloudimg-x64-20170621"
#  region: [region]
#  dns_zone_internal: ""
#  dns_zone_external: "{%- if dns_tld_external -%}  [region].{{app_class}}.{{buildenv}}.{{dns_tld_external}}  {%- endif -%}"
#  dns_server: "openstack"    # Specify DNS server. infoblox or openstack (on openstack) supported.  If empty string is specified, no DNS will be added.
#  dev:
#    hosttype_vars:
#      sys: {count_per_az: 1, az: ["az1", "az2"], flavor: m1.1c.1g, rootvol_size: "10"}
#  network: "[team_name]_{{buildenv}}"
#  security_group: "wide-open"
#  key_name: "{{os_username}}"
#  floating_ip_pools: "vlan3321"
#  OS_AUTH_URL:
#  OS_TENANT_ID:
#  OS_TENANT_NAME:
#  OS_PROJECT_NAME:
