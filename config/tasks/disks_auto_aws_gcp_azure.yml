---

- name: disks_auto_aws_gcp_azure | cluster_hosts_target(inventory_hostname)
  debug: msg={{ cluster_hosts_target | json_query(\"[?hostname == '\" + inventory_hostname + \"'] \") }}

- name: disks_auto_aws_gcp_azure | Mount block devices as individual disks
  block:
    - name: disks_auto_aws_gcp_azure | auto_vols
      debug: msg={{ auto_vols }}

    - name: disks_auto_aws_gcp_azure | Get the block device information (pre-filesystem create)
      blockdevmap:
        cloud_type: "{{cluster_vars.type}}"
      become: yes
      register: r__blockdevmap

    - name: disks_auto_aws_gcp_azure | r__blockdevmap (pre-filesystem create)
      debug: msg={{r__blockdevmap}}

    - name: disks_auto_aws_gcp_azure | Create filesystem (partitionless)
      become: yes
      filesystem:
        fstype: "{{ item.fstype }}"
        dev: "{{ _dev }}"
      loop: "{{auto_vols}}"
      vars:
        _dev: "{{ r__blockdevmap.device_map | json_query(\"[?device_name_cloud == '\" + item.device_name + \"' && TYPE=='disk' && parttable_type=='' && FSTYPE=='' && MOUNTPOINT==''].device_name_os | [0]\") }}"
      when: _dev is defined and _dev != ''

    - name: disks_auto_aws_gcp_azure | Get the block device information (post-filesystem create), to get the block IDs for mounting
      blockdevmap:
        cloud_type: "{{cluster_vars.type}}"
      become: yes
      register: r__blockdevmap

    - name: disks_auto_aws_gcp_azure | r__blockdevmap (post-filesystem create)
      debug: msg={{r__blockdevmap}}

    - name: disks_auto_aws_gcp_azure | Mount created filesytem(s) persistently
      become: yes
      mount:
        path: "{{ item.mountpoint }}"
        src: "UUID={{ _UUID }}"
        fstype: "{{ item.fstype }}"
        state: mounted
        opts: _netdev
      loop: "{{auto_vols}}"
      vars:
        _UUID: "{{ r__blockdevmap.device_map | json_query(\"[?device_name_cloud == '\" + item.device_name + \"' && TYPE=='disk' && parttable_type=='' && MOUNTPOINT==''].UUID | [0]\") }}"
      when: _UUID is defined and _UUID != ''

    - name: disks_auto_aws_gcp_azure | change ownership of mountpoint (if set)
      become: yes
      file:
        path: "{{ item.mountpoint }}"
        state: directory
        mode: "{{ item.perms.mode | default(omit)}}"
        owner: "{{ item.perms.owner | default(omit)}}"
        group: "{{ item.perms.group | default(omit)}}"
      loop: "{{auto_vols}}"

    - name: disks_auto_aws_gcp_azure | Check that we haven't mounted disks in the wrong place.  Especially useful for redeploys when we're moving disks.
      block:
        - name: "disks_auto_aws_gcp_azure | Touch a file with the mountpoint and device name for testing that disk attachment is correct.  Note: Use a unique filename here instead of writing to a file, so that more than one file per device is an error.  Note: don't add device_name for GCP, because we can't rename the disks when redeploying and keeping disks (_scheme_rmvm_keepdisk_rollback)"
          become: yes
          file:
            path: "{{item.mountpoint}}/.clusterversetest__{{inventory_hostname | regex_replace('-(?!.*-).*')}}__{{ item.mountpoint | regex_replace('\\/', '_') }}{%- if cluster_vars.type != 'gcp'-%}__{{ item.device_name | regex_replace('\/', '_') }}{%- endif -%}"
            state: touch
          loop: "{{auto_vols}}"

        - name: disks_auto_aws_gcp_azure | Find all .clusterversetest__ files in mounted disks
          find:
            paths: "{{item.mountpoint}}"
            hidden: yes
            patterns: ".clusterversetest__*"
          loop: "{{auto_vols}}"
          register: r__find_test

        - name: disks_auto_aws_gcp_azure | Check that there is only one .clusterversetest__ file per device in mounted disks.
          block:
            - name: disks_auto_aws_gcp_azure | testdevicedescriptor
              debug: msg={{testdevicedescriptor}}

            - name: disks_auto_aws_gcp_azure | assert that only one device descriptor file exists per disk (otherwise, indicates that this run has mapped either more than one device per mount, or a different one to previous)
              assert: { that: "testdevicedescriptor | json_query(\"[?length(files) > `1`]\") | length == 0", fail_msg: "ERROR - Exactly one file should exist per storage device.  In error [{{testdevicedescriptor | json_query(\"[?length(files) > `1`]\")}}]" }
          vars:
            testdevicedescriptor: "{{ r__find_test | json_query(\"results[].{hostname: '\" + inventory_hostname + \"', device_name: item.device_name, mountpoint: item.mountpoint, files: files[].path}\") }}"
      when: test_touch_disks is defined and test_touch_disks|bool
  when: (auto_vols | map(attribute='mountpoint') | list | unique | count == auto_vols | map(attribute='mountpoint') | list | count)
  vars:
    auto_vols: "{{ cluster_hosts_target | json_query(\"[?hostname == '\" + inventory_hostname + \"'].auto_volumes[]\") }}"


# The following block mounts all attached volumes that have a single, common mountpoint, by creating a logical volume
- name: disks_auto_aws_gcp_azure/lvm | Mount block devices in a single LVM mountpoint through LV/VG
  block:
    - name: disks_auto_aws_gcp_azure/lvm | raid_vols
      debug: msg={{ raid_vols }}

    - name: disks_auto_aws_gcp_azure/lvm | Install logical volume management tooling. (yum - RedHat/CentOS)
      become: true
      yum:
        name: "lvm*"
        state: present
      when: ansible_os_family == 'RedHat'

    - name: disks_auto_aws_gcp_azure/lvm | Get the device information (pre-filesystem create)
      blockdevmap:
        cloud_type: "{{cluster_vars.type}}"
      become: yes
      register: r__blockdevmap

    - name: disks_auto_aws_gcp_azure/lvm | r__blockdevmap (pre raid create)
      debug: msg={{r__blockdevmap}}

    - block:
        - name: disks_auto_aws_gcp_azure/lvm | raid_vols_devices
          debug: msg={{ raid_vols_devices }}

        - name: disks_auto_aws_gcp_azure/lvm | Create a volume group from all block devices
          become: yes
          lvg:
            vg:  "{{ lvmparams.vg_name }}"
            pvs: "{{ raid_vols_devices | map(attribute='device_name_os') | sort  | join(',') }}"
    
        - name: disks_auto_aws_gcp_azure/lvm | Create a logical volume from volume group
          become: yes
          lvol:
            vg: "{{ lvmparams.vg_name }}"
            lv: "{{ lvmparams.lv_name }}"
            size: "{{ lvmparams.lv_size }}"
    
        - name: disks_auto_aws_gcp_azure/lvm | Create filesystem(s) on attached volume(s)
          become: yes
          filesystem:
            fstype: "{{ raid_vols[0].fstype }}"
            dev: "/dev/{{ lvmparams.vg_name }}/{{ lvmparams.lv_name }}"
            force: no
    
        - name: disks_auto_aws_gcp_azure/lvm | Mount created filesytem(s) persistently
          become: yes
          mount:
            path: "{{ raid_vols[0].mountpoint }}"
            src: "/dev/{{ lvmparams.vg_name }}/{{ lvmparams.lv_name }}"
            fstype: "{{ raid_vols[0].fstype }}"
            state: mounted
            opts: _netdev
    
        - name: disks_auto_aws_gcp_azure/lvm | Check that we haven't mounted disks in the wrong place.  Especially useful for redeploys when we're moving disks.
          block:
            - name: "disks_auto_aws_gcp_azure/lvm | Touch a file with the mountpoint for testing that disk attachment is correct.  Note: Use a unique filename here instead of writing to a file, so that more than one file per device is an error."
              become: yes
              file:
                path: "{{ raid_vols[0].mountpoint }}/.clusterversetest__{{inventory_hostname | regex_replace('-(?!.*-).*')}}__{{ raid_vols[0].mountpoint | regex_replace('\\/', '_') }}"
                state: touch
    
            - name: disks_auto_aws_gcp_azure/lvm | Find all .clusterversetest__ files in mounted disks
              find:
                paths: "{{ raid_vols[0].mountpoint }}"
                hidden: yes
                patterns: ".clusterversetest__*"
              register: r__find_test
    
            - debug: msg={{r__find_test}}
    
            - name: disks_auto_aws_gcp_azure/lvm | assert that only one device descriptor file exists per disk (otherwise, indicates that this run has mapped either more than one device per mount, or a different one to previous)
              assert: { that: "'files' in r__find_test != ''  and  r__find_test.files | length == 1", fail_msg: "ERROR - Exactly one file should exist per LVM." }
          when: test_touch_disks is defined and test_touch_disks|bool
      vars:
        raid_vols_devices: "{{ r__blockdevmap.device_map | json_query(\"[?device_name_cloud && contains('\" + (raid_vols | map(attribute='device_name') | sort  | join(',')) + \"', device_name_cloud)]\") }}"
      when: raid_vols_devices | length

  when: (lvmparams is defined and lvmparams != {})  and  (raid_vols | map(attribute='mountpoint') | list | unique | count == 1) and (raid_vols | map(attribute='mountpoint') | list | count >= 2) and (raid_vols | map(attribute='fstype') | list | unique | count == 1)
  vars:
    _hosttype_vars: "{{ cluster_hosts_target | json_query(\"[?hostname == '\" + inventory_hostname + \"'] | [0]\") }}"
    raid_vols: "{{ (_hosttype_vars.auto_volumes | selectattr('mountpoint', '!=', '/') | default([])) if _hosttype_vars.auto_volumes is defined else [] }}"
    lvmparams: "{{ (cluster_vars[buildenv].hosttype_vars[_hosttype_vars.hosttype].lvmparams | default({})) if _hosttype_vars.hosttype is defined else {} }}"
